{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import legendre\n",
    "from numpy.linalg import svd\n",
    "\n",
    "from time import time\n",
    "\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def superposition(data):\n",
    "    scale=StandardScaler()\n",
    "    \n",
    "\n",
    "    scaled = scale.fit_transform(data)\n",
    "    sum=np.zeros((5000,5000))\n",
    "    x = np.linspace(-1,1,5000)\n",
    "    for i in range(data.shape[1]):\n",
    "        sum=sum+np.outer(data[:,i],legendre(i+1)(x))\n",
    "    \n",
    "    return sum\n",
    "\n",
    "def singularvaluedecomposition(data):\n",
    "    data_sup = superposition(data)\n",
    "    a, data_svd, b = svd(data_sup)\n",
    "\n",
    "    return data_svd\n",
    "\n",
    "def normalize_matrix(matrix):\n",
    "    matrix_norm = np.linalg.norm(matrix, 'fro')\n",
    "\n",
    "    if np.isnan(matrix_norm):\n",
    "        return np.zeros_like(matrix)\n",
    "    else:\n",
    "        normalized_matrix = matrix / matrix_norm\n",
    "        return normalized_matrix\n",
    "    \n",
    "def coarse_grain(data,f):\n",
    "    img = normalize_matrix(superposition(data))\n",
    "    s1=np.zeros((int(img.shape[0]/f),int(img.shape[1]/f))) # Coarse grained image\n",
    "    for i in range(s1.shape[0]):\n",
    "        for j in range(s1.shape[1]):\n",
    "            s1[i,j]=np.mean(img[i*f:i*f+f,j*f:j*f+f]) \n",
    "\n",
    "    return s1\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# data = load('/home/lumi/Documents/Python/CODES/data_prep/sample_disease.npy')\n",
    "# cc = coarse_grain(data[0],50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('junk.txt',cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m cc \u001b[38;5;241m=\u001b[39m norm_matrix\n\u001b[1;32m     26\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m---> 27\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoarse_grain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m end \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(i, end\u001b[38;5;241m-\u001b[39m start)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "    \n",
    "#### SR data\n",
    "\n",
    "data = load('/home/lumi/Documents/Python/CODES/data_prep/disease_SR_v1.npy')\n",
    "print(len(data))\n",
    "split_number = 10\n",
    "if len(data)%split_number == 0:\n",
    "    data_split = np.array_split(data,split_number)\n",
    "else:\n",
    "    data_split = np.array_split(data[0:len(data)-(len(data)%split_number)], split_number)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(split_number):\n",
    "    cc = data_split[i]\n",
    "\n",
    "    norm_matrix = []\n",
    "    for j in range(len(cc)):\n",
    "        norm_matrix.append(normalize_matrix(cc[j]))\n",
    "\n",
    "        \n",
    "\n",
    "    cc = norm_matrix\n",
    "\n",
    "    start = time()\n",
    "    results = Parallel(n_jobs = 5)(delayed(coarse_grain)(x,50) for x in cc)\n",
    "    end = time()\n",
    "\n",
    "    print(i, end- start)\n",
    "\n",
    "    results = np.array(results)\n",
    "    cc = []\n",
    "    for j in range(results.shape[0]):\n",
    "        cc.append(results[j].ravel())\n",
    "\n",
    "    np.savetxt('disease_SR_svd' + '_'+str(i) + '.txt',cc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0981786251068115\n",
      "1.076186180114746\n",
      "1.0964772701263428\n",
      "1.1466643810272217\n",
      "1.1190502643585205\n",
      "1.1406655311584473\n",
      "1.136946678161621\n",
      "1.1727440357208252\n",
      "1.1279160976409912\n",
      "1.1170103549957275\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "    \n",
    "#### SB data\n",
    "    \n",
    "data = load('/home/lumi/Documents/Python/CODES/data_prep/disease_SB_v1.npy')\n",
    "\n",
    "split_number = 10\n",
    "if len(data)%split_number == 0:\n",
    "    data_split = np.array_split(data,split_number)\n",
    "else:\n",
    "    data_split = np.array_split(data[0:len(data)-(len(data)%split_number)], split_number)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(split_number):\n",
    "    cc = data_split[i]\n",
    "\n",
    "    norm_matrix = []\n",
    "    for j in range(len(cc)):\n",
    "        norm_matrix.append(normalize_matrix(cc[j]))\n",
    "\n",
    "    cc = norm_matrix\n",
    "\n",
    "    start = time()\n",
    "    results = Parallel(n_jobs = 10)(delayed(coarse_grain)(x,50) for x in cc)\n",
    "    end = time()\n",
    "\n",
    "    print(end- start)\n",
    "    results = np.array(results)\n",
    "    cc = []\n",
    "    for j in range(results.shape[0]):\n",
    "        cc.append(results[j].ravel())\n",
    "    np.savetxt('disease_SB_svd' + '_'+str(i) + '.txt',cc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0015594959259033203\n",
      "0.0005121231079101562\n",
      "0.00027298927307128906\n",
      "0.00018906593322753906\n",
      "0.00017952919006347656\n",
      "0.00019216537475585938\n",
      "0.0001952648162841797\n",
      "0.0001842975616455078\n",
      "0.0001652240753173828\n",
      "0.00017571449279785156\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "    \n",
    "#### SB data\n",
    "    \n",
    "data = load('/home/lumi/Documents/Python/CODES/data_prep/disease_ST_v1.npy')\n",
    "\n",
    "split_number = 10\n",
    "if len(data)%split_number == 0:\n",
    "    data_split = np.array_split(data,split_number)\n",
    "else:\n",
    "    data_split = np.array_split(data[0:len(data)-(len(data)%split_number)], split_number)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(split_number):\n",
    "    cc = data_split[i]\n",
    "\n",
    "    norm_matrix = []\n",
    "    for j in range(len(cc)):\n",
    "        norm_matrix.append(normalize_matrix(cc[j]))\n",
    "\n",
    "    cc = norm_matrix\n",
    "    \n",
    "    start = time()\n",
    "    results = Parallel(n_jobs = 10)(delayed(coarse_grain)(x,50) for x in cc)\n",
    "    end = time()\n",
    "\n",
    "    print(end- start)\n",
    "\n",
    "    results = np.array(results)\n",
    "    cc = []\n",
    "    for j in range(results.shape[0]):\n",
    "        cc.append(results[j].ravel())\n",
    "    np.savetxt('disease_ST_svd' + '_'+str(i) + '.txt',cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
